{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X8RbiUWIzVcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8333c4-e58f-4e21-c31f-03a39e72277d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/carlolepelaars/toy-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.13M/1.13M [00:00<00:00, 1.60MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/carlolepelaars/toy-dataset/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"carlolepelaars/toy-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(texts):\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    return sequences, tokenizer"
      ],
      "metadata": {
        "id": "OrAz7oqdPa-o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences, input_tokenizer = tokenize(input_texts)\n",
        "target_sequences, target_tokenizer = tokenize(target_texts)\n"
      ],
      "metadata": {
        "id": "V8mAi5BIPzVG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_sequences)\n",
        "print(target_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwGWaZbDQGYD",
        "outputId": "43902398-5390-408f-8202-7cf311a31d2a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 3, 4, 5, 2], [1, 6, 7, 8, 2], [1, 9, 10, 11, 2]]\n",
            "[[1, 3, 4, 5, 2], [1, 6, 7, 8, 2], [1, 9, 10, 11, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = pad_sequences(input_sequences, padding='post')\n",
        "target_tensor = pad_sequences(target_sequences, padding='post')"
      ],
      "metadata": {
        "id": "2vqvnhx3P_oj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_tensor)\n",
        "print(\"target_tensor\",target_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVYJPVTwQneu",
        "outputId": "0125fcb3-6440-4cb9-ae1e-79f3f0b70b3e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  3  4  5  2]\n",
            " [ 1  6  7  8  2]\n",
            " [ 1  9 10 11  2]]\n",
            "target_tensor [[ 1  3  4  5  2]\n",
            " [ 1  6  7  8  2]\n",
            " [ 1  9 10 11  2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Input Vocabulary Size:\", len(input_tokenizer.word_index) + 1)\n",
        "print(\"Target Vocabulary Size:\", len(target_tokenizer.word_index) + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtHV0EoeQwAM",
        "outputId": "b7c8d16d-a43c-4f58-82a1-42010ce66f4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input Vocabulary Size: 12\n",
            "Target Vocabulary Size: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQA7NMBmRBvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}